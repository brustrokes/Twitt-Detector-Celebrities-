{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n",
      "                                            Mensagem  \\\n",
      "0  Four years ago  @MichelleObama and I had the p...   \n",
      "1  I got my start holding community meetings in C...   \n",
      "2  Young people have helped lead all our great mo...   \n",
      "3  Billy Graham was a humble servant who prayed f...   \n",
      "4  We are grieving with Parkland. But we are not ...   \n",
      "5  Happy Valentine’s Day  @MichelleObama. You mak...   \n",
      "6  RT @MichelleObama: Happy #ValentinesDay to my ...   \n",
      "7  Dr. King was 26 when the Montgomery bus boycot...   \n",
      "8  All across America people chose to get involve...   \n",
      "9  Ten-year-old Jahkil Jackson is on a mission to...   \n",
      "\n",
      "                             date author  \n",
      "0  Mon Mar 12 18:12:41 +0000 2018  Obama  \n",
      "1  Wed Feb 28 02:58:41 +0000 2018  Obama  \n",
      "2  Thu Feb 22 16:00:44 +0000 2018  Obama  \n",
      "3  Wed Feb 21 16:33:04 +0000 2018  Obama  \n",
      "4  Thu Feb 15 17:12:17 +0000 2018  Obama  \n",
      "5  Wed Feb 14 16:25:03 +0000 2018  Obama  \n",
      "6  Wed Feb 14 16:24:22 +0000 2018  Obama  \n",
      "7  Mon Jan 15 14:46:02 +0000 2018  Obama  \n",
      "8  Fri Dec 29 16:11:11 +0000 2017  Obama  \n",
      "9  Fri Dec 29 16:10:36 +0000 2017  Obama  \n"
     ]
    }
   ],
   "source": [
    "messages_data = pd.read_csv(\"twitts/message.csv\",encoding='utf-8')\n",
    "print \"Dados carregados com sucesso!\"\n",
    "print(messages_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musk\n",
      "200\n",
      "Obama\n",
      "200\n",
      "Trump\n",
      "200\n",
      "Gates\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "print('Musk')\n",
    "print(sum(messages_data['author'] == 'Musk'))\n",
    "print('Obama')\n",
    "print(sum(messages_data['author'] == 'Obama'))\n",
    "print('Trump')\n",
    "print(sum(messages_data['author'] == 'Trump'))\n",
    "print('Gates')\n",
    "print(sum(messages_data['author'] == 'Gates'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from time import strptime\n",
    "\n",
    "\n",
    "dates_in_string =[]\n",
    "dates = messages_data['date']\n",
    "i = 0\n",
    "for m in dates:\n",
    "    #print(m)\n",
    "    month = strptime(m[4:7],'%b').tm_mon\n",
    "    day =  m[8:10]\n",
    "    year = m[26:30]\n",
    "    aut = messages_data['author'][i]\n",
    "    #if(aut == 'Obama'):\n",
    "        #dates_in_string.append(dt.datetime.strptime(, \"%d%m%Y\").date())\n",
    "    i = i + 1\n",
    "    #print(aut)\n",
    "print(dates_in_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brustrokes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'stops_words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-461b83f476de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# import words to set with stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mwords_to_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stops_words.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m def plot_confusion_matrix(cm, classes,\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'stops_words.txt'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# import words to set with stop\n",
    "words_to_stop = open(\"stops_words.txt\", \"r\").read().replace(\"\\n\",\",\").split(\",\")\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"orange\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, \"lxml\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # add words in list to set with stop\n",
    "    stops.update((words_to_stop))\n",
    "    \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   \n",
    "\n",
    "clean_review = review_to_words( \"Just @ test to see the review\" )\n",
    "print clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split  \n",
    "\n",
    "author = messages_data['author']\n",
    "message = messages_data['Mensagem']\n",
    "MensagemArray = []\n",
    "i = 0\n",
    "for m in message:\n",
    "    #remove URL from messages\n",
    "    m = re.sub(r\"http\\S+\", \"\", m)\n",
    "    #MensagemArray.append([review_to_words(m),spam_id[i] ])\n",
    "    MensagemArray.append(review_to_words(m))\n",
    "    i = i + 1\n",
    "\n",
    "features = messages_data.drop(['Mensagem','date'], axis = 1)\n",
    "#print(MensagemArray.shape)\n",
    "# TODO: Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(MensagemArray, features ,test_size = 0.15, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn import svm\n",
    "\n",
    "pipe_LogisticRegression = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_NaiveBayes = make_pipeline(GaussianNB())\n",
    "pipe_NearestCentroid = make_pipeline(NearestCentroid())\n",
    "pipe_SVM = make_pipeline(svm.LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.2)\n",
    "data_train_transformed = vectorizer.fit_transform(X_train)\n",
    "data_test_transformed  = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=5)\n",
    "selector.fit(data_train_transformed, y_train)\n",
    "data_train_transformed = selector.transform(data_train_transformed).toarray()\n",
    "data_test_transformed  = selector.transform(data_test_transformed).toarray()\n",
    "\n",
    "\n",
    "\n",
    "pipe_LogisticRegression.fit(data_train_transformed, y_train)\n",
    "pipe_NaiveBayes.fit(data_train_transformed, y_train)\n",
    "pipe_NearestCentroid.fit(data_train_transformed, y_train)\n",
    "pipe_SVM.fit(data_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions =pipe_LogisticRegression.predict(data_test_transformed)\n",
    "predito_treino = pipe_LogisticRegression.predict(data_test_transformed)# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=1)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['gates','musk','obama','trump'], normalize=True,  title='Confusion matrix Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions =pipe_NaiveBayes.predict(data_test_transformed)\n",
    "predito_treino = pipe_NaiveBayes.predict(data_test_transformed)# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=1)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['gates','musk','obama','trump'], normalize=True,  title='Confusion matrix Naive Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions =pipe_NearestCentroid.predict(data_test_transformed)\n",
    "predito_treino = pipe_NearestCentroid.predict(data_test_transformed)# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=1)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['gates','musk','obama','trump'], normalize=True,  title='Confusion matrix Nearest Centroid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions =pipe_SVM.predict(data_test_transformed)\n",
    "predito_treino = pipe_SVM.predict(data_test_transformed)# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=1)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['gates','musk','obama','trump'], normalize=True,  title='Confusion matrix SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#message = 'Who wants a tesla in a space 9 ship?'\n",
    "message = 'my wife michelle recive young leaders to help then to build the future'\n",
    "#message = 'We need to build an wall to keep mexicans off'\n",
    "#message = 'Who wants to help to keep malaria off Africa?' \n",
    "message = 'Advances in agriculture can have a big impact on the lives of the world’s poorest. I’m particularly excited about new research from the @RIPEproject.'\n",
    "\n",
    "vectorize_message = vectorizer.transform([message])\n",
    "vectorize_message = selector.transform(vectorize_message).toarray()\n",
    "predict_proba = pipe.predict_proba(vectorize_message).tolist()\n",
    "predict_proba[0][0] = (\"%.4f\" % predict_proba[0][0])\n",
    "predict_proba[0][1] = (\"%.4f\" % predict_proba[0][1])\n",
    "predict_proba[0][2] = (\"%.4f\" % predict_proba[0][2])\n",
    "predict_proba[0][3] = (\"%.4f\" % predict_proba[0][3])\n",
    "predict = pipe_SVM.predict(vectorize_message).tolist()\n",
    "print(predict)\n",
    "print(predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('spam/pipe_TwitterDetector.dill', 'wb') as file:\n",
    "    dill.dump(pipe, file)\n",
    "    \n",
    "with open('spam/pipe_vectorizer_TwitterDetector.dill', 'wb') as file:\n",
    "    dill.dump(vectorizer, file)\n",
    "with open('spam/pipe_selector_TwitterDetector.dill', 'wb') as file:\n",
    "    dill.dump(selector, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para recuperar a predição é necessário executar apenas os codigos abaixo em um ambiente para rapida analise da mensagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import dill\n",
    "\n",
    "import os\n",
    "from flask import Flask, jsonify, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('spam/pipe_TwitterDetector.dill', 'rb') as f:\n",
    "    spam_clf = dill.load(f)\n",
    "with open('spam/pipe_vectorizer_TwitterDetector.dill', 'rb') as f:\n",
    "    spam_vectorizer = dill.load(f)   \n",
    "with open('spam/pipe_selector_TwitterDetector.dill', 'rb') as f:\n",
    "    spam_selector = dill.load(f)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Musk']\n",
      "[['0.0028', '0.9971', 3.121481835008155e-07, 0.000153340881528573]]\n"
     ]
    }
   ],
   "source": [
    "message = 'I will send my space 9 to mars!'\n",
    "vectorize_message = spam_vectorizer.transform([message])\n",
    "vectorize_message = spam_selector.transform(vectorize_message).toarray()\n",
    "predict_proba = spam_clf.predict_proba(vectorize_message).tolist()\n",
    "predict_proba[0][0] = (\"%.4f\" % predict_proba[0][0])\n",
    "predict_proba[0][1] = (\"%.4f\" % predict_proba[0][1])\n",
    "predict = spam_clf.predict(vectorize_message).tolist()\n",
    "print(predict)\n",
    "print(predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/twitter_detector', methods=['POST'])\n",
    "def spam():\n",
    "    json_ = request.json   \n",
    "    message = json_['message']\n",
    "    vectorize_message = spam_vectorizer.transform([message])\n",
    "    vectorize_message = spam_selector.transform(vectorize_message).toarray()\n",
    "    predict_proba = spam_clf.predict_proba(vectorize_message).tolist()\n",
    "    predict_proba[0][0] = (\"%.4f\" % predict_proba[0][0])\n",
    "    predict_proba[0][1] = (\"%.4f\" % predict_proba[0][1])\n",
    "    predict = spam_clf.predict(vectorize_message).tolist()\n",
    "    responses = jsonify({'predict':predict,'predict_proba':predict_proba})\n",
    "    responses.status_code = 200\n",
    "    return (responses)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
