{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n",
      "                                            Mensagem  \\\n",
      "0  Four years ago  @MichelleObama and I had the p...   \n",
      "1  I got my start holding community meetings in C...   \n",
      "2  Young people have helped lead all our great mo...   \n",
      "3  Billy Graham was a humble servant who prayed f...   \n",
      "4  We are grieving with Parkland. But we are not ...   \n",
      "5  Happy Valentineâ€™s Day  @MichelleObama. You mak...   \n",
      "6  RT @MichelleObama: Happy #ValentinesDay to my ...   \n",
      "7  Dr. King was 26 when the Montgomery bus boycot...   \n",
      "8  All across America people chose to get involve...   \n",
      "9  Ten-year-old Jahkil Jackson is on a mission to...   \n",
      "\n",
      "                             date author  \n",
      "0  Mon Mar 12 18:12:41 +0000 2018  Obama  \n",
      "1  Wed Feb 28 02:58:41 +0000 2018  Obama  \n",
      "2  Thu Feb 22 16:00:44 +0000 2018  Obama  \n",
      "3  Wed Feb 21 16:33:04 +0000 2018  Obama  \n",
      "4  Thu Feb 15 17:12:17 +0000 2018  Obama  \n",
      "5  Wed Feb 14 16:25:03 +0000 2018  Obama  \n",
      "6  Wed Feb 14 16:24:22 +0000 2018  Obama  \n",
      "7  Mon Jan 15 14:46:02 +0000 2018  Obama  \n",
      "8  Fri Dec 29 16:11:11 +0000 2017  Obama  \n",
      "9  Fri Dec 29 16:10:36 +0000 2017  Obama  \n"
     ]
    }
   ],
   "source": [
    "messages_data = pd.read_csv(\"twitts/message.csv\",encoding='utf-8')\n",
    "print \"Dados carregados com sucesso!\"\n",
    "print(messages_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musk\n",
      "200\n",
      "Obama\n",
      "200\n",
      "Trump\n",
      "200\n",
      "Gates\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "print('Musk')\n",
    "print(sum(messages_data['author'] == 'Musk'))\n",
    "print('Obama')\n",
    "print(sum(messages_data['author'] == 'Obama'))\n",
    "print('Trump')\n",
    "print(sum(messages_data['author'] == 'Trump'))\n",
    "print('Gates')\n",
    "print(sum(messages_data['author'] == 'Gates'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump:30 days, 0:00:00\n",
      "Musk:66 days, 0:00:00\n",
      "Gates:249 days, 0:00:00\n",
      "Obama:531 days, 0:00:00\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from time import strptime\n",
    "\n",
    "\n",
    "Obama_dates_in_string =[]\n",
    "Trump_dates_in_string =[]\n",
    "Musk_dates_in_string =[]\n",
    "Gates_dates_in_string =[]\n",
    "dates = messages_data['date']\n",
    "i = 0\n",
    "for m in dates:\n",
    "    #print(m)\n",
    "    month = strptime(m[4:7],'%b').tm_mon\n",
    "    day =  m[8:10]\n",
    "    year = m[26:30]\n",
    "    aut = messages_data['author'][i]\n",
    "    if(aut == 'Obama'):\n",
    "        Obama_dates_in_string.append(dt.datetime.strptime(day+str(month)+year, \"%d%m%Y\").date())\n",
    "    if(aut == 'Trump'):\n",
    "        Trump_dates_in_string.append(dt.datetime.strptime(day+str(month)+year, \"%d%m%Y\").date())\n",
    "    if(aut == 'Musk'):\n",
    "        Musk_dates_in_string.append(dt.datetime.strptime(day+str(month)+year, \"%d%m%Y\").date())\n",
    "    if(aut == 'Gates'):\n",
    "        Gates_dates_in_string.append(dt.datetime.strptime(day+str(month)+year, \"%d%m%Y\").date())\n",
    "    i = i + 1\n",
    "    #print(aut)\n",
    "\n",
    "TumpinDays = max(Trump_dates_in_string) - min(Trump_dates_in_string)\n",
    "print('Trump:' + str(TumpinDays))\n",
    "MuskinDays = max(Musk_dates_in_string) - min(Musk_dates_in_string)\n",
    "print('Musk:' + str(MuskinDays))\n",
    "GatesinDays = max(Gates_dates_in_string) - min(Gates_dates_in_string)\n",
    "print('Gates:' + str(GatesinDays))\n",
    "ObamainDays = max(Obama_dates_in_string) - min(Obama_dates_in_string)\n",
    "print('Obama:' + str(ObamainDays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d%m%Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "plt.plot(Gates_dates_in_string,range(len(Gates_dates_in_string)))\n",
    "plt.plot(Trump_dates_in_string,range(len(Trump_dates_in_string)))\n",
    "plt.plot(Musk_dates_in_string,range(len(Musk_dates_in_string)))\n",
    "plt.plot(Obama_dates_in_string,range(len(Obama_dates_in_string)))\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brustrokes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "test see review\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# import words to set with stop\n",
    "words_to_stop = open(\"stops_words.txt\", \"r\").read().replace(\"\\n\",\",\").split(\",\")\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"orange\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, \"lxml\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # add words in list to set with stop\n",
    "    stops.update((words_to_stop))\n",
    "    \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   \n",
    "\n",
    "clean_review = review_to_words( \"Just @ test to see the review\" )\n",
    "print clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\brustrokes\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split  \n",
    "\n",
    "author = messages_data['author']\n",
    "message = messages_data['Mensagem']\n",
    "MensagemArray = []\n",
    "i = 0\n",
    "for m in message:\n",
    "    #remove URL from messages\n",
    "    m = re.sub(r\"http\\S+\", \"\", m)\n",
    "    #MensagemArray.append([review_to_words(m),spam_id[i] ])\n",
    "    MensagemArray.append(review_to_words(m))\n",
    "    i = i + 1\n",
    "\n",
    "features = messages_data.drop(['Mensagem','date'], axis = 1)\n",
    "#print(MensagemArray.shape)\n",
    "# TODO: Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(MensagemArray, features ,test_size = 0.15, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wordcloud.wordcloud.WordCloud object at 0x00000000111D6EB8>\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "import matplotlib as mpl\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#mpl.rcParams['figure.figsize']=(8.0,6.0)    #(6.0,4.0)\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "    \n",
    "wordcloud = WordCloud().generate(str(MensagemArray))\n",
    "\n",
    "print(wordcloud)\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "fig.savefig(\"word1.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1)\n"
     ]
    }
   ],
   "source": [
    "print features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn import svm\n",
    "\n",
    "pipe_LogisticRegression = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_NaiveBayes = make_pipeline(GaussianNB())\n",
    "pipe_NearestCentroid = make_pipeline(NearestCentroid())\n",
    "pipe_SVM = make_pipeline(svm.LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\brustrokes\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('linearsvc', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.2)\n",
    "data_train_transformed = vectorizer.fit_transform(X_train)\n",
    "data_test_transformed  = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=5)\n",
    "selector.fit(data_train_transformed, y_train)\n",
    "data_train_transformed = selector.transform(data_train_transformed).toarray()\n",
    "data_test_transformed  = selector.transform(data_test_transformed).toarray()\n",
    "\n",
    "\n",
    "\n",
    "pipe_LogisticRegression.fit(data_train_transformed, y_train)\n",
    "pipe_NaiveBayes.fit(data_train_transformed, y_train)\n",
    "pipe_NearestCentroid.fit(data_train_transformed, y_train)\n",
    "pipe_SVM.fit(data_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8  0.2  0.   0. ]\n",
      " [ 0.1  0.9  0.   0. ]\n",
      " [ 0.1  0.1  0.8  0. ]\n",
      " [ 0.1  0.1  0.   0.8]]\n"
     ]
    }
   ],
   "source": [
    "predictions =pipe_LogisticRegression.predict(data_test_transformed)\n",
    "predito_treino = pipe_LogisticRegression.predict(data_test_transformed)# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=1)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['gates','musk','obama','trump'], normalize=True,  title='Confusion matrix Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8  0.2  0.   0. ]\n",
      " [ 0.   1.   0.   0. ]\n",
      " [ 0.1  0.1  0.7  0.1]\n",
      " [ 0.1  0.1  0.1  0.7]]\n"
     ]
    }
   ],
   "source": [
    "predictions =pipe_NaiveBayes.predict(data_test_transformed)\n",
    "predito_treino = pipe_NaiveBayes.predict(data_test_transformed)# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=1)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['gates','musk','obama','trump'], normalize=True,  title='Confusion matrix Naive Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8  0.2  0.   0. ]\n",
      " [ 0.1  0.9  0.   0. ]\n",
      " [ 0.1  0.1  0.7  0.1]\n",
      " [ 0.2  0.1  0.   0.7]]\n"
     ]
    }
   ],
   "source": [
    "predictions =pipe_NearestCentroid.predict(data_test_transformed)\n",
    "predito_treino = pipe_NearestCentroid.predict(data_test_transformed)# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=1)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['gates','musk','obama','trump'], normalize=True,  title='Confusion matrix Nearest Centroid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8  0.2  0.   0. ]\n",
      " [ 0.1  0.9  0.   0. ]\n",
      " [ 0.   0.1  0.8  0.1]\n",
      " [ 0.1  0.1  0.   0.8]]\n"
     ]
    }
   ],
   "source": [
    "predictions =pipe_SVM.predict(data_test_transformed)\n",
    "predito_treino = pipe_SVM.predict(data_test_transformed)# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=1)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['gates','musk','obama','trump'], normalize=True,  title='Confusion matrix SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Musk']\n",
      "[['0.2378', '0.5211', '0.1589', '0.0822']]\n"
     ]
    }
   ],
   "source": [
    "#message = 'Who wants a tesla in a space 9 ship?'\n",
    "message = 'my wife michelle recive young leaders to help then to build the future'\n",
    "#message = 'We need to build an wall to keep mexicans off'\n",
    "#message = 'Who wants to help to keep malaria off Africa?' \n",
    "message = 'Advances in agriculture can have a big impact on the lives of the worldâ€™s poorest. Iâ€™m particularly excited about new research from the @RIPEproject.'\n",
    "message = 'Have fun out there among the stars.'\n",
    "vectorize_message = vectorizer.transform([message])\n",
    "vectorize_message = selector.transform(vectorize_message).toarray()\n",
    "predict_proba = pipe_LogisticRegression.predict_proba(vectorize_message).tolist()\n",
    "predict_proba[0][0] = (\"%.4f\" % predict_proba[0][0])\n",
    "predict_proba[0][1] = (\"%.4f\" % predict_proba[0][1])\n",
    "predict_proba[0][2] = (\"%.4f\" % predict_proba[0][2])\n",
    "predict_proba[0][3] = (\"%.4f\" % predict_proba[0][3])\n",
    "predict = pipe_LogisticRegression.predict(vectorize_message).tolist()\n",
    "print(predict)\n",
    "print(predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('dill/pipe_TwitterDetector.dill', 'wb') as file:\n",
    "    dill.dump(pipe_LogisticRegression, file)\n",
    "    \n",
    "with open('dill/pipe_vectorizer_TwitterDetector.dill', 'wb') as file:\n",
    "    dill.dump(vectorizer, file)\n",
    "with open('dill/pipe_selector_TwitterDetector.dill', 'wb') as file:\n",
    "    dill.dump(selector, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para recuperar a prediÃ§Ã£o Ã© necessÃ¡rio executar apenas os codigos abaixo em um ambiente para rapida analise da mensagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import dill\n",
    "\n",
    "import os\n",
    "from flask import Flask, jsonify, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dill/pipe_TwitterDetector.dill', 'rb') as f:\n",
    "    twitt_clf = dill.load(f)\n",
    "with open('dill/pipe_vectorizer_TwitterDetector.dill', 'rb') as f:\n",
    "    spam_vectorizer = dill.load(f)   \n",
    "with open('dill/pipe_selector_TwitterDetector.dill', 'rb') as f:\n",
    "    spam_selector = dill.load(f)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Musk']\n",
      "[['0.0053', '0.9932', 8.686146341514894e-06, 0.0014865971772851155]]\n"
     ]
    }
   ],
   "source": [
    "message = 'I will send my space 9 to mars!'\n",
    "vectorize_message = spam_vectorizer.transform([message])\n",
    "vectorize_message = spam_selector.transform(vectorize_message).toarray()\n",
    "predict_proba = twitt_clf.predict_proba(vectorize_message).tolist()\n",
    "predict_proba[0][0] = (\"%.4f\" % predict_proba[0][0])\n",
    "predict_proba[0][1] = (\"%.4f\" % predict_proba[0][1])\n",
    "predict_proba[0][2] = (\"%.4f\" % predict_proba[0][2])\n",
    "predict_proba[0][3] = (\"%.4f\" % predict_proba[0][3])\n",
    "predict = twitt_clf.predict(vectorize_message).tolist()\n",
    "print(predict)\n",
    "print(predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/twitter_detector', methods=['POST'])\n",
    "def spam():\n",
    "    json_ = request.json   \n",
    "    message = json_['message']\n",
    "    vectorize_message = spam_vectorizer.transform([message])\n",
    "    vectorize_message = spam_selector.transform(vectorize_message).toarray()\n",
    "    predict_proba = spam_clf.predict_proba(vectorize_message).tolist()\n",
    "    predict_proba[0][0] = (\"%.4f\" % predict_proba[0][0])\n",
    "    predict_proba[0][1] = (\"%.4f\" % predict_proba[0][1])\n",
    "    predict_proba[0][2] = (\"%.4f\" % predict_proba[0][2])\n",
    "    predict_proba[0][3] = (\"%.4f\" % predict_proba[0][3])\n",
    "    predict = twitt_clf.predict(vectorize_message).tolist()\n",
    "    responses = jsonify({'predict':predict,'predict_proba':predict_proba})\n",
    "    responses.status_code = 200\n",
    "    return (responses)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
